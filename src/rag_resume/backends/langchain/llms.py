from typing import Any, assert_never, final

from langchain.schema import AIMessage, BaseMessage, SystemMessage
from langchain_core.language_models import BaseChatModel
from langchain_core.messages import HumanMessage
from langchain_core.runnables import RunnableLambda

from rag_resume.json import JsonCodecProtocol
from rag_resume.llms.chat import ChatLLMProtocol, ChatMessage, ChatRole


def convert_to_langchain_message(chat_message: ChatMessage) -> HumanMessage | SystemMessage | AIMessage:
    """Converts a ChatMessage to a corresponding message type in the langchain library.

    Args:
        chat_message (ChatMessage): The ChatMessage object to convert.
    ss
    Returns:
        Union[HumanMessage, SystemMessage, AIMessage]: The converted message.
    """
    match chat_message.role:
        case ChatRole.USER:
            return HumanMessage(content=chat_message.content)
        case ChatRole.SYSTEM:
            return SystemMessage(content=chat_message.content)
        case ChatRole.ASSISTANT:
            return AIMessage(content=chat_message.content)
        case _:
            assert_never(chat_message)


def convert_response_to_chat_message(chat_message: BaseMessage) -> ChatMessage:
    """Converts a ChatMessage to a corresponding message type in the langchain library.

    Args:
        chat_message (BaseMessage): The ChatMessage object to convert.

    Returns:
        Union[HumanMessage, SystemMessage, AIMessage]: The converted message.
    """
    match chat_message:
        case AIMessage() as message:
            return ChatMessage(
                role=ChatRole.ASSISTANT,
                content=message.content,
                response_metadata=message.response_metadata,
                id=message.id,
                usage_metadata=dict(message.usage_metadata) if message.usage_metadata else None,
            )
        case _:
            return ChatMessage(
                role=ChatRole.ASSISTANT,
                content=chat_message.content,
                response_metadata=chat_message.response_metadata,
                id=chat_message.id,
            )


def _extract_base_message_from_structured(message: dict[str, Any]) -> BaseMessage:
    return message["raw"]


@final
class LangChainChatLLM(ChatLLMProtocol):
    """Wrapper for langchain LLMs to match internal protocols."""

    def __init__(
        self, lang_chain_model: BaseChatModel, structured_output: dict[str, Any] | JsonCodecProtocol | None = None
    ) -> None:
        self.chat_llm = lang_chain_model
        self.runnable = self.chat_llm
        self.codec = None
        self.structured_output = structured_output

    def chat(self, messages: list[ChatMessage]) -> ChatMessage:
        """Sends a list of chat messages to the language model and returns a response message.

        Args:
            messages (list[ChatMessage]): A list of chat messages to be sent to the language model.

        Returns:
            ChatMessage: The response message generated by the language model.
        """
        message = [convert_to_langchain_message(msg) for msg in messages]
        return convert_response_to_chat_message(self.runnable.invoke(message))

    async def async_chat(self, messages: list[ChatMessage]) -> ChatMessage:
        """Asynchronously sends a list of chat messages to the language model and returns a response message.

        Args:
            messages (list[ChatMessage]): A list of chat messages to be sent to the language model.

        Returns:
            ChatMessage: The response message generated by the language model.
        """
        message = [convert_to_langchain_message(msg) for msg in messages]
        return convert_response_to_chat_message(await self.runnable.ainvoke(message))

    @property
    def structured_output[StructuredTypeVar](self) -> dict[str, Any] | None:
        """Property for structured output for LLM."""
        return self._structured_output

    @structured_output.setter
    def structured_output(self, schema: dict[str, Any] | JsonCodecProtocol | None) -> dict[str, Any] | None:
        match schema:
            case dict():
                self._structured_output = schema
                self.runnable = self.chat_llm.with_structured_output(
                    self._structured_output, include_raw=True
                ) | RunnableLambda(_extract_base_message_from_structured)
            case JsonCodecProtocol():
                self._structured_output = schema.schema()
                self.runnable = self.chat_llm.with_structured_output(
                    self._structured_output, include_raw=True
                ) | RunnableLambda(_extract_base_message_from_structured)
            case None:
                self._structured_output = None
                self.runnable = self.chat_llm
        return self._structured_output
